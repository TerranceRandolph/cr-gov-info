{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USC CODE START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas  as gpd\n",
    "import os\n",
    "import json\n",
    "from sqlalchemy import engine\n",
    "# from PIL import Image\n",
    "# from PIL.ExifTags import TAGS\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import gdal2tiles  # not installed on USC computers\n",
    "import xml.etree.ElementTree as ET\n",
    "from shapely import wkt\n",
    "from shapely.validation import make_valid\n",
    "import glob\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNC COVERAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanbytes(B):\n",
    "    \"\"\"Return the given bytes as a human friendly KB, MB, GB, or TB string.\"\"\"\n",
    "    B = float(B)\n",
    "    KB = float(1024)\n",
    "    MB = float(KB ** 2) # 1,048,576\n",
    "    GB = float(KB ** 3) # 1,073,741,824\n",
    "    TB = float(KB ** 4) # 1,099,511,627,776\n",
    "\n",
    "    if B < KB:\n",
    "        return '{0} {1}'.format(B,'Bytes' if 0 == B > 1 else 'Byte')\n",
    "    elif KB <= B < MB:\n",
    "        return '{0:.2f} KB'.format(B / KB)\n",
    "    elif MB <= B < GB:\n",
    "        return '{0:.2f} MB'.format(B / MB)\n",
    "    elif GB <= B < TB:\n",
    "        return '{0:.2f} GB'.format(B / GB)\n",
    "    elif TB <= B:\n",
    "        return '{0:.2f} TB'.format(B / TB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_coverage(env, img_path, cov_path):\n",
    "    # need to call/open command line which uses gdal cli functions to process coverages\n",
    "    # change directiory\n",
    "    os.chdir(env)\n",
    "    # indexes are coverages i.e coverage creation from tiff to new folder\n",
    "    cmd = f'gdaltindex {cov_path} {img_path}'\n",
    "    subprocess.call(cmd,shell=True)\n",
    "    # print(f'coverage created -- {cov_path}')\n",
    "    return(cov_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coverages(folder,env):\n",
    "    # ------------------------------------------------- #\n",
    "    dfdict = {'img_name':[], 'fips':[],'dt_year':[],'img_num':[], 'img_size':[],\n",
    "            'meta_size':[],'meta_unit':[],'state':[],'meta_last_acc':[],'img_path':[]}\n",
    "    # ------------------------------------------------- #\n",
    "    dfdict_cov_geom = {'img_name':[], 'area_sqkm':[], 'shape':[]}\n",
    "    # ------------------------------------------------- #\n",
    "    for image in glob.glob(os.path.join(folder, '*.tif')):\n",
    "        image = image.replace('\\\\','/')\n",
    "        # ------------------------------------------------- #\n",
    "        img_path    = os.path.join(folder, image)\n",
    "        stats       = os.stat(img_path)\n",
    "        # temp path for coverage geojson\n",
    "        # create a temporary folder using library tempfile\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        temp_cov_path = os.path.join(temp_dir, 'temp_coverage.geojson')\n",
    "        # print(temp_cov_path)\n",
    "        # ------------------------------------------------- #\n",
    "        # get the image name without extension\n",
    "        # img_name = image.split('\\\\')[-1].replace('.tif','') ; print(img_name)\n",
    "        img_name   = os.path.basename(image).replace('.tif','') \n",
    "        imgsplit    = img_name.split('_')\n",
    "        # ------------------------------------------------- #\n",
    "        (fips, dt_year, img_num, img_size,\n",
    "            meta_size, meta_lastmod, meta_lastacc) = (imgsplit[0], imgsplit[1],\n",
    "                                                    imgsplit[3], imgsplit[5], humanbytes(round(stats.st_size)),\n",
    "                                                        datetime.fromtimestamp(stats.st_mtime),\n",
    "                                                        datetime.fromtimestamp(stats.st_atime))\n",
    "        # ------------------------------------------------- #\n",
    "        # make coverages image outlines vector version\n",
    "        cov_path        = mk_coverage(env, img_path, temp_cov_path)\n",
    "        crs_string      = 'EPSG:3857'\n",
    "        # coverages are now a spatial file that needs tiff metadata added to it via appending a list\n",
    "        gdf             = gpd.read_file(cov_path, crs=crs_string)\n",
    "        # precautionary check for odd polys -- not needed \n",
    "        gdf             = gdf.dissolve()\n",
    "        # double checking and making sure geometry is valid -- not need but good practice\n",
    "        gdf['geometry'] = gdf['geometry'].apply(make_valid)\n",
    "        gdf['area_sqkm']= gdf['geometry'].map(lambda p: p.area / 10**6)\n",
    "        sqkm            = int(round(sum(gdf.area_sqkm),2))\n",
    "        geom            = gdf['geometry'].values[0]\n",
    "        # ------------------------------------------------- #\n",
    "        dfdict['img_name'].append(str(img_name))      ;  dfdict['fips'].append(str(fips)),\n",
    "        dfdict['dt_year'].append(str(dt_year))        ;  dfdict['img_num'].append(str(img_num)),\n",
    "        dfdict['img_size'].append(str(img_size))      ;  dfdict['meta_size'].append(str(meta_size).replace(' MB',''))\n",
    "        dfdict['meta_last_acc'].append(str(meta_lastacc))  ;  dfdict['img_path'].append(str(img_path))\n",
    "        dfdict['meta_unit'].append('MB')                   ;  dfdict['state'].append('South Carolina')\n",
    "        # ------------------------------------------------- #\n",
    "        dfdict_cov_geom['img_name'].append(str(img_name))\n",
    "        dfdict_cov_geom['area_sqkm'].append(sqkm)#(int(round(sum(gdf.area_sqkm),2)))\n",
    "        dfdict_cov_geom['shape'].append(geom)\n",
    "    # merge both dictionaries on img_name\n",
    "    merge_dict = {**dfdict, **dfdict_cov_geom}\n",
    "    return(pd.DataFrame(merge_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if a file exists, if not create it\n",
    "def ck_file(x):\n",
    "    if not os.path.exists(x):os.makedirs(x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(xml_path, minzoom, maxzoom):\n",
    "    # read xml file\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    # get the name of the image\n",
    "    name = root.find('Title').text\n",
    "    # get the bounds of the image\n",
    "    bounds = root.find('BoundingBox').attrib\n",
    "    # get the minx, miny, maxx, maxy\n",
    "    minx = bounds['minx']\n",
    "    miny = bounds['miny']\n",
    "    maxx = bounds['maxx']\n",
    "    maxy = bounds['maxy']\n",
    "    # create a dictionary\n",
    "    json_dict = {'name':name, 'version':'1.0.0','description':'', 'type':'overlay',\n",
    "                 'format':'png','minzoom':minzoom, 'maxzoom':maxzoom, 'bounds':f'{minx},{miny},{maxx},{maxy}',\n",
    "                'scale':'1', 'profile':'mercator'}\n",
    "    return(json_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish global vars for county, years, cov folder name etc..\n",
    "county         = 'D:\\\\45085-Sumter' #G:\\\\45013-Beaufort'\n",
    "yr_folder_list = glob.glob(f'{county}/*') # var contents ex : ['G:\\\\45013-Beaufort\\\\45013_TIFF_1939', 'G:\\\\45013-Beaufort\\\\45013_TIFF_1941']\n",
    "env            = 'C:\\\\Users\\\\terra\\\\anaconda3\\\\envs\\\\pygdal' # need to call/open command line which uses gdal cli functions to process coverages, mosaics/merges and tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in yr_folder_list:\n",
    "    # check if year is a folder\n",
    "    if not os.path.isdir(year):continue\n",
    "    # coverages folder path\n",
    "    cov_path = ck_file(os.path.join(year,'coverage_by_image'))\n",
    "    # call function to create coverages and store in dataframe\n",
    "    df  = create_coverages(year,env)\n",
    "    # make the df spatial\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=df['shape'], crs=3857)\n",
    "    gdf = gdf.drop(columns=['shape'])\n",
    "    # make a dissolved/unioned version\n",
    "    gdf_new = gdf.dissolve(by='state')\n",
    "    # store coverages in appropriate folder\n",
    "    gdf.to_file(os.path.join(cov_path,'coverages.geojson'), driver=\"GeoJSON\") \n",
    "    gdf_new.to_file(os.path.join(cov_path,'coverages_dissolved.geojson'), driver=\"GeoJSON\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 to 8 bit Merge/Mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\45085-Sumter\\45085_TIFF_1980\\45085_1980_179_0005_x_9.tif D:\\45085-Sumter\\45085_TIFF_1980\\45085_1980_179_0007_x_9.tif\n",
      "vertual file True\n",
      "merged file True\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://www.youtube.com/watch?v=sBBMKbAj8XE\n",
    "# using the second method because it create a vertual environment that won't bog down memory\n",
    "# hopefully no need for chunking\n",
    "# os.chdir(env)\n",
    "for year in yr_folder_list:\n",
    "    os.chdir(env)\n",
    "    # ------------------------------------------------- #\n",
    "    if not os.path.isdir(year):continue\n",
    "    # create a merged geotiff folder\n",
    "    merged_tiff     = ck_file(os.path.join(year,'merged_tiff'))\n",
    "    merged_tiff_txt = os.path.join(merged_tiff,'merged_tiff_list.txt')\n",
    "    merged_tiff_vrt = os.path.join(merged_tiff,'merged_tiff.vrt')\n",
    "    merged_geotiff  = os.path.join(merged_tiff,'merged_geotiff.tif')\n",
    "    # list all tiffs in yr folder and store as merged_tiff_list.txt\n",
    "    # purpose is to use the list as refference when updating the merged geotif and not reprocess all tiffs\n",
    "    # check if merged_tiff_list.txt exists, if not create it\n",
    "    # --- #\n",
    "    org_tiff_list = glob.glob(f\"{year}/*.tif\")\n",
    "    # ------------------------------------------------- #\n",
    "    if not os.path.exists(merged_tiff_txt):\n",
    "        # list all tiffs in yr folder\n",
    "        tiff_list = glob.glob(f\"{year}/*.tif\")\n",
    "        # write tiff_list to txt file\n",
    "        with open(merged_tiff_txt, 'w') as f:\n",
    "            for item in tiff_list:\n",
    "                f.write('%s\\n' % item)\n",
    "    # ------------------------------------------------- #\n",
    "    else:\n",
    "        # read in the txt file and store as tiff list to be used to update the merged geotif\n",
    "        with open(merged_tiff_txt, 'r') as f:\n",
    "            tiff_list = f.read().splitlines()\n",
    "    # ------------------------------------------------- # \n",
    "    # UPDATE CLAUSE: compare the org_tiff_list to the tiff_list if there are any new tiffs, create var new_tiff_list\n",
    "    s = set(tiff_list)\n",
    "    new_tiff_list = [x for x in org_tiff_list if x not in s]\n",
    "    # check if geotiff exists, if it does, rename it to merged_geotiff_old.tif and update new tiffs list to include it\n",
    "    if os.path.exists(merged_geotiff):\n",
    "        os.rename(merged_geotiff,merged_geotiff.replace('.tif','_old.tif'))\n",
    "        if merged_geotiff.replace('.tif','_old.tif') not in new_tiff_list:\n",
    "            new_tiff_list.append(merged_geotiff.replace('.tif','_old.tif'))\n",
    "    # convert to string in order to use gdalbuildvrt\n",
    "    tiff_string = ' '.join(new_tiff_list[:2])\n",
    "    print(tiff_string)\n",
    "    # --- #\n",
    "    # gdalbuildvrt to create a vrt file from the tiff list\n",
    "    cmdV = f'''gdalbuildvrt -srcnodata 0 -vrtnodata 0 -overwrite \"{merged_tiff_vrt}\" {tiff_string}'''\n",
    "    # subprocess.call(cmdV,shell=True)\n",
    "    time.sleep(1)\n",
    "    print(f'vertual file {os.path.exists(merged_tiff_vrt)}')\n",
    "    # merge the vertual file to a webp compressed geotiff in 8bit format\n",
    "    cmdM = f'''gdal_translate -a_nodata 0.0 -ot Byte -of GTiff {merged_tiff_vrt.replace('\"','')} {merged_geotiff.replace('\"','')}'''\n",
    "    # subprocess.call(cmdM,shell=True) # 3 files took 11mins to process; however updating  with 2 tiffs and 1 merged geotiff took 3mins\n",
    "    time.sleep(2)\n",
    "    print(f'merged file {os.path.exists(merged_geotiff)}')\n",
    "    # ------------------------------------------------- #\n",
    "    # update text file with new tiffs overwrite the old one\n",
    "    with open(merged_tiff_txt, 'a') as f:\n",
    "        tiffs = '\\n'.join(new_tiff_list+'\\n'+tiff_list)\n",
    "        f.write(tiffs)\n",
    "    # ------------------------------------------------- #\n",
    "    # remove the old merged geotiff because it is no longer needed\n",
    "    shutil.rmtree(merged_geotiff.replace('.tif','_old.tif'), ignore_errors=True)\n",
    "    print('old merged geotiff removed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile & Make Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in yr_folder_list:\n",
    "    tile_cov        = ck_file(os.path.join(year,'tiled_coverages'))\n",
    "    merged_geotiff  = os.path.join(merged_tiff,'merged_geotiff.tif')\n",
    "    json_file       = os.path.join(tile_cov,'tilemapresource.json')\n",
    "    xml_file        = os.path.join(tile_cov,'tilemapresource.xml')\n",
    "    # UPDATE CLAUSE : check if tilemapresource.json exists, if so empty the folder and reprocess\n",
    "    if os.path.exists(json_file):\n",
    "        shutil.rmtree(tile_cov)\n",
    "        os.mkdir(tile_cov)\n",
    "    # source: https://gdal2tiles.readthedocs.io/en/latest/readme.html\n",
    "    gdal2tiles.generate_tiles(merged_geotiff, tile_cov, zoom='2-5', processes=4,\n",
    "                              s_srs='EPSG:3857', profile='mercator', resampling='average',\n",
    "                              webp=True)\n",
    "    # make a json file for the tileset\n",
    "    j = create_json(xml_file, 2, 5)\n",
    "    with open(json_file, 'w') as outfile:\n",
    "        json.dump(j, outfile)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # f'''gdal2tiles.bat -p mercator -z 8-10 -w all -r average -a 0.0\n",
    "    #         \"{out_merge}\" \"{tile_path}\"'''\n",
    "    # begin merging tiffs process\n",
    "    # build vertual raster that a mosaic of all tiffs in tiff_list\n",
    "    # gdal.BuildVRT( dst file: xml file that stores the list of tiffs,\n",
    "    #                src file: list of tiffs,\n",
    "    #                separate: True means each tiff is a band, False means all tiffs are one band,\n",
    "    #                 all tiffs have multiple bands so separate=True)\n",
    "    # vrt = gdal.BuildVRT(merged_tiff_vrt, tiff_list)\n",
    "    # print('vertual raster created')\n",
    "    \n",
    "    # gdal.Translate( dst file: merged geotiff,\n",
    "    #                src file: vrt,\n",
    "    #             format: geotiff,\n",
    "    #             compress 16 bit to 8 bit\n",
    "    # gdal.Translate(merged_geotiff, vrt, format='GTiff')\n",
    "    # print('geotiff created')\n",
    "    # close vrt\n",
    "    # vrt = None\n",
    "    # print('vrt closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def tiff_to_tile(folder):\n",
    "#     bit_webp_ls     = []\n",
    "#     need_process    = []\n",
    "#     txt_ls          = []\n",
    "#     # -------------------- #\n",
    "#     # create a temp directory to store 8bit.webp files\n",
    "#     temp_dir = tempfile.mkdtemp()\n",
    "#     # -------------------- #\n",
    "#     # list of tiff files\n",
    "#     tiff_ls = [t.replace('\\\\','/') for t in glob.glob(os.path.join(folder, '*.tif'))]\n",
    "#     tiff_ls = [t.split('/')[-1].replace('.tif','') for t in tiff_ls]\n",
    "#     # -------------------- #\n",
    "    \n",
    "#     # read list from text file\n",
    "#     with open(os.path.join(folder,'cov_8bit/bit_webp_ls.txt'), 'r') as f:\n",
    "#         for line in f:\n",
    "#             txt_ls.append(line.split(',')) \n",
    "#     # flatten list\n",
    "#     txt_ls = [item for sublist in txt_ls for item in sublist]  \n",
    "#     # print(f'list from text file: {txt_ls}')\n",
    "#     # print(f'list of tiff files: {tiff_ls}')\n",
    "    \n",
    "#     # -------------------- #\n",
    "    \n",
    "#     # if tiff names are not in txt file, add to need_process list\n",
    "#     if txt_ls not in tiff_ls:\n",
    "#         # append list of tiff files that are not in txt file\n",
    "#         need_process.append([x for x in tiff_ls if x not in txt_ls])\n",
    "#     # flatten list\n",
    "#     need_process = [item for sublist in need_process for item in sublist]\n",
    "#     # print(f'list of tiff files that need to be processing: {need_process}')\n",
    "    \n",
    "#     # -------------------- #\n",
    "    \n",
    "#     for image in need_process: \n",
    "#         # folders and files var establishment\n",
    "#         img_name = image.split('/')[-1].replace('.tif','')\n",
    "#         org_tif  = os.path.join(folder, f'{img_name}.tif')\n",
    "#         bit_webp   = os.path.join(temp_dir, f'{img_name}_8bit.webp')\n",
    "#         # -------------------- #\n",
    "#         # print(f'processing {img_name}...')\n",
    "#         # convert tiff to 8bit webp format\n",
    "#         cmd_8bit = f'gdal_translate -ot Byte -scale {org_tif} {bit_webp}'\n",
    "#         # converte tiff to 8bit webp format\n",
    "#         subprocess.call(cmd_8bit,shell=True)\n",
    "#         # append to list of 8bit.webp files\n",
    "#         bit_webp_ls.append(img_name)  \n",
    "#         # print(f'{img_name} completed')\n",
    "    \n",
    "#     # -------------------- #\n",
    "    \n",
    "#     # overwrite current bit_webp_ls.txt & save list of 8bit.webp files to a text file for future use\n",
    "#     with open(os.path.join(folder,'cov_8bit/bit_webp_ls.txt'), 'w') as f:\n",
    "#         # write each item from the list to text file\n",
    "#         for item in bit_webp_ls:\n",
    "#             f.write(\"%s\" % item)\n",
    "#     print(f'list of 8bit.webp files: {bit_webp_ls}') \n",
    "#     ## SOLVE FOR REFRESHING THE LIST OF 8BIT.WEBP FILES ##\n",
    "    \n",
    "#     ## FINAL VERSIION NEEDS VARS FOR 3 IN-YEAR FOLDERS ##\n",
    "    \n",
    "#     # -------------------- #\n",
    "#     # mosaic all 8bit.webp files in temp_dir\n",
    "    \n",
    "#     # list of all 8bit.webp files in temp_dir\n",
    "#     webp_files = glob.glob(os.path.join(temp_dir, '*.webp'))\n",
    "#     tile_folder = os.path.join(folder,'mosaic').replace('\\\\','/')\n",
    "#     # gdal mosaic all files in temp folder\n",
    "#     cmd_mosaic = f'''gdalbuildvrt -overwrite -resolution average\n",
    "#      -r nearest -input_file_list {webp_files} {tile_folder}/mosaic.vrt'''\n",
    "#     subprocess.call(cmd_mosaic,shell=True) \n",
    "#     print(f'mosaic.vrt created\\n{tile_folder}')\n",
    "#     # print(webp_files)\n",
    "    \n",
    "#     ## TROUBLESHOOT WHY gdalbuildvrt IS NOT WORKING ##\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# tiff_to_tile(folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge 'nodata' not set BYTE (8BIT) output 0 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gdal_merge.bat -a_nodata 0.0 -ot Byte -of GTiff -o\n",
    "         \"C:/Users/ArcGIS Student/AppData/Local/Temp/processing_zlStPW/5b9bddc4b2ae459d8f60525152222319/OUTPUT.tif\" \n",
    "--optfile \"{out}\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get list of files files.. \n",
    "# file_list = glob.glob(\"c:\\data\\....\\*.tif\")\n",
    "\n",
    "# files_string = \" \".join(file_list)\n",
    "# # code needed to batch list based on large number of files, i.e list greater than 100 batch by increase by 1 for every 50 then dump temp file memory.\n",
    "# out_merge    = # merge_image_folder\\\\merge*date YY-MM-dd\n",
    "\n",
    "# merge_cmd = f'''gdal_merge.bat -a_nodata 0.0 -ot Byte -of GTiff \n",
    "#             -o         {files_string}\n",
    "#             --optfile \"{out_merge}\"\n",
    "#             '''\n",
    "\n",
    "\n",
    "# # once merge is completed, tile merge in tile folder\n",
    "# tile_path = # tile_folder\\\\tiled_*date YY-MM-dd\n",
    "\n",
    "\n",
    "# # tile command\n",
    "# tile_cmd = f'''gdal2tiles.bat -p mercator -z 8-10 -w all -r average -a 0.0\n",
    "#             \"{out_merge}\" \"{tile_path}\"'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gdal2tiles.bat -p mercator -z 8-10 -w all -r average -a 0.0 \"C:/Users/ArcGIS Student/AppData/Local/Temp/processing_zlStPW/f5bca6e8f38c4881b3ad500030680668/OUTPUT.tif\" \"C:/Users/ArcGIS Student/AppData/Local/Temp/processing_zlStPW/2bfcee2fbc4a41f996931ed107d0ee7e/OUTPUT\"'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16bit to 8bit Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for yr_path in yr_folder_list:\n",
    "# \t# build recursive list of tiffs in the folder\n",
    "# \tmy_list = glob.glob(f'{yr_path}\\\\*.tif')\n",
    "# \t# chunck list into manageable bites \n",
    "# \tchunk_size = 35 # 35 was chosen at random and will later be based on folder size and processing will later be parallelized as my previous version was.\n",
    "# \twhile my_list:\n",
    "# \t\tchunk, my_list = my_list[:chunk_size], my_list[chunk_size:]\n",
    "# \t\tfor num, path in enumerate(chunk):\n",
    "# \t\t# temp path and stuff here\n",
    "# \t\t\tprint(num, path)\n",
    "# \t\t# outside of loop... this is where the mk_mosaic will live\n",
    "# \t\tprint('new')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to merge/mosaic files -- NOTE:: check algorthms behind both to ensure merging is the best method\n",
    "# tif_ls = ['G:\\\\45013-Beaufort\\\\45013_TIFF_1979\\\\45013_1979_0178_135L_x_24.tif','G:\\\\45013-Beaufort\\\\45013_TIFF_1979\\\\45013_1979_0178_135R_x_24.tif', 'G:\\\\45013-Beaufort\\\\45013_TIFF_1979\\\\45013_1979_0178_136R_x_24.tif']\n",
    "# mosaic_path = 'F:\\\\45085-Sumter'\n",
    "# # def mk_mosaic(env, tif_ls, mosaic_path):\n",
    "#     # need to call/open command line which uses gdal cli functions to process coverages\n",
    "#     # change directiory\n",
    "# os.chdir(env)\n",
    "# # indexes are coverages i.e coverage creation from tiff to new folder\n",
    "# cmd = f'gdal_merge.py -a_nodata 0.0 -ot Byte -of GTiff -o {mosaic_path} --optfile {tif_ls}'   # f'gdaltindex {cov_path} {img_path}'\n",
    "# subprocess.call(cmd,shell=True)\n",
    "#     # print(f'coverage created -- {cov_path}')\n",
    "#     # return(cov_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdal_merge.py -n 0 -v -o mosaic_31.tif --optfile tiff_list.txt\n",
    "\n",
    "# gdal_merge.bat -a_nodata 0.0 -ot Byte -of GTiff \n",
    "#             -o         {files_string}\n",
    "#             --optfile \"{out_merge}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# env     = 'C:/Users/ArcGIS Student/anaconda3/envs/geospatial_23'#'D:/County_fips/fips_year'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygdal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a7f5f7dfa8e25979ed39dd627cd3a16c652c463c14d6268420180b6342db461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
